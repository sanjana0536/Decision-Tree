{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- **Decision Tree in Classification**\n",
        "\n",
        "A Decision Tree is a type of supervised learning algorithm used for classification and regression tasks. It's a tree-like model that splits data into subsets based on features or attributes.\n",
        "\n",
        "**How Decision Trees Work**\n",
        "1. Root Node: The algorithm starts with a root node representing the entire dataset.\n",
        "2. Splitting: The algorithm selects the best feature to split the data into subsets based on a splitting criterion (e.g., Gini impurity or entropy).\n",
        "3. Decision Nodes: Each internal node represents a decision based on a feature, and the data is split into subsets accordingly.\n",
        "4. Leaf Nodes: The terminal nodes represent the predicted class labels.\n",
        "5. Prediction: New instances are classified by traversing the tree from the root node to a leaf node based on the feature values.\n",
        "\n",
        "**Decision Tree Example**\n",
        "\n",
        "Suppose we want to classify whether someone will play tennis based on weather conditions (outlook, temperature, humidity, and wind).\n",
        "\n",
        "| Outlook | Temperature | Humidity | Wind | Play Tennis |\n",
        "| ------- | ----------- | -------- | ---- | ----------- |\n",
        "| Sunny | Hot | High | Weak | No |\n",
        "| Sunny | Hot | High | Strong | No |\n",
        "| Overcast | Hot | High | Weak | Yes |\n",
        "| Rain | Mild | High | Weak | Yes |\n",
        "\n",
        "\n",
        "The Decision Tree might look like this:\n",
        "\n",
        "- Root Node: Outlook\n",
        "    - Sunny: Humidity\n",
        "        - High: No\n",
        "        - Normal: Yes\n",
        "    - Overcast: Yes\n",
        "    - Rain: Wind\n",
        "        - Weak: Yes\n",
        "        - Strong: No\n",
        "\n",
        "**Advantages**\n",
        "- Interpretability: Decision Trees are easy to understand and visualize.\n",
        "- Handling categorical features: Decision Trees can handle categorical features directly.\n",
        "\n",
        "**Disadvantages**\n",
        "- Overfitting: Decision Trees can overfit the training data, especially when the trees are deep.\n",
        "- Instability: Small changes in the data can result in significantly different trees.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p-hAQSTmjvMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8C1nB6KbW-Jr"
      },
      "outputs": [],
      "source": [
        "# Python Implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "# This example demonstrates how to create and train a Decision Tree classifier using scikit-learn in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- **Impurity Measures in Decision Tr**ees\n",
        "\n",
        "Gini Impurity and Entropy are two common impurity measures used in Decision Trees to determine the best split.\n",
        "\n",
        "- **Gini ImpurityGini**\n",
        "\n",
        "Impurity measures the probability of misclassifying a randomly chosen instance from a dataset. It's calculated as:\n",
        "\n",
        "Gini Impurity = 1 - Σ (p_i)^2\n",
        "\n",
        "where p_i is the proportion of instances of class i in the dataset.\n",
        "\n",
        "- **Entropy**\n",
        "\n",
        "Entropy measures the uncertainty or randomness in a dataset. It's calculated as:\n",
        "\n",
        "Entropy = - Σ (p_i * log2(p_i))\n",
        "\n",
        "where p_i is the proportion of instances of class i in the dataset.\n",
        "\n",
        "- **Impact on Splits**\n",
        "\n",
        "Both Gini Impurity and Entropy are used to evaluate the quality of a split. The goal is to find the split that results in the largest reduction in impurity.\n",
        "\n",
        "1. Gini Impurity: A lower Gini Impurity value indicates a purer node. Decision Trees using Gini Impurity tend to favor splits that result in large, pure nodes.\n",
        "2. Entropy: A lower Entropy value indicates less uncertainty in the node. Decision Trees using Entropy tend to favor splits that result in nodes with low uncertainty.\n",
        "\n",
        "**Comparison**\n",
        "- Gini Impurity: Faster to compute and often produces similar results to Entropy.\n",
        "- Entropy: More sensitive to changes in class probabilities and can lead to more nuanced splits.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have a dataset with two classes (0 and 1) and a feature that splits the data into two subsets:\n",
        "\n",
        "Subset 1: 80% class 0, 20% class 1\n",
        "\n",
        "Subset 2: 40% class 0, 60% class 1\n",
        "\n",
        "Using Gini Impurity:\n",
        "\n",
        "Gini Impurity (Subset 1) = 1 - (0.8)^2 - (0.2)^2 = 0.32\n",
        "\n",
        "Gini Impurity (Subset 2) = 1 - (0.4)^2 - (0.6)^2 = 0.48\n",
        "\n",
        "Using Entropy:\n",
        "\n",
        "Entropy (Subset 1) = -(0.8 * log2(0.8) + 0.2 * log2(0.2)) ≈ 0.72\n",
        "\n",
        "Entropy (Subset 2) = -(0.4 * log2(0.4) + 0.6 * log2(0.6)) ≈ 0.97\n",
        "\n",
        "The Decision Tree would choose the split that results in the largest reduction in impurity."
      ],
      "metadata": {
        "id": "3zEcl-wJYRPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree classifier using Gini Impurity\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Create and train a Decision Tree classifier using Entropy\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# This example demonstrates how to create and train Decision Tree classifiers using Gini Impurity and Entropy as impurity measures in scikit-learn."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "tqlwxQ37Y1RQ",
        "outputId": "06b1260a-f4aa-4dce-9a1d-28658ab1ac8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- **Pre-Pruning vs Post-Pruning in Decision Trees**\n",
        "\n",
        "Pre-Pruning and Post-Pruning are two techniques used to prevent overfitting in Decision Trees.\n",
        "\n",
        "**Pre-Pruning**\n",
        "\n",
        "Pre-Pruning involves stopping the growth of the tree before it's fully grown. This is done by specifying a set of parameters that control the tree's growth, such as:\n",
        "\n",
        "- Maximum depth: The maximum number of levels in the tree.\n",
        "- Minimum samples split: The minimum number of samples required to split an internal node.\n",
        "- Minimum samples leaf: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "**Post-Pruning**\n",
        "\n",
        "Post-Pruning involves growing the tree to its full depth and then removing branches that don't contribute significantly to the tree's performance. This is done by:\n",
        "\n",
        "- Cost complexity pruning: Removing branches that have a high cost complexity parameter.\n",
        "\n",
        "**Practical Advantages**\n",
        "\n",
        "- Pre-Pruning: One practical advantage of pre-pruning is that it reduces computational cost. By stopping the growth of the tree early, we can avoid unnecessary computations and reduce the risk of overfitting.\n",
        "- Post-Pruning: One practical advantage of post-pruning is that it allows for more flexibility. By growing the tree to its full depth, we can explore more possible splits and then prune the tree to remove unnecessary branches, potentially leading to better performance.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have a dataset with 1000 samples and 10 features. We can use pre-pruning to limit the maximum depth of the tree to 5 levels, or we can use post-pruning to grow the tree to its full depth and then remove branches with high cost complexity.\n"
      ],
      "metadata": {
        "id": "u5FRLU13aGGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pre-Pruning: Limit the maximum depth to 5\n",
        "clf_pre_pruning = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "clf_pre_pruning.fit(X_train, y_train)\n",
        "\n",
        "# Post-Pruning: Use cost complexity pruning\n",
        "path = clf_pre_pruning.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "clf_post_pruning = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alphas[-1])\n",
        "clf_post_pruning.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# This example demonstrates how to implement pre-pruning and post-pruning in scikit-learn."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "-Pmb21eSbGcb",
        "outputId": "5912c59f-8fc6-4b86-b349-4bbb6b9f5026"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=np.float64(0.3334027777777779),\n",
              "                       random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(ccp_alpha=np.float64(0.3334027777777779),\n",
              "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(ccp_alpha=np.float64(0.3334027777777779),\n",
              "                       random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- **Information Gain in Decision Trees**\n",
        "\n",
        "Information Gain is a measure used in Decision Trees to evaluate the quality of a split. It's calculated as the difference in entropy between the parent node and the child nodes.\n",
        "\n",
        "Information Gain = Entropy (Parent) - Σ (|Child| / |Parent|) * Entropy (Child)\n",
        "\n",
        "where |Child| is the number of instances in the child node, and |Parent| is the number of instances in the parent node.\n",
        "\n",
        "**Importance for Choosing the Best Split**\n",
        "\n",
        "Information Gain is important for choosing the best split because it helps to:\n",
        "\n",
        "1. Reduce uncertainty: By choosing the split with the highest Information Gain, we can reduce the uncertainty in the child nodes and make more accurate predictions.\n",
        "2. Identify relevant features: Information Gain helps to identify the most relevant features for splitting the data, which can improve the overall performance of the Decision Tree.\n",
        "\n",
        "**How Information Gain Works**\n",
        "\n",
        "1. Calculate entropy: Calculate the entropy of the parent node and the child nodes.\n",
        "2. Calculate Information Gain: Calculate the Information Gain for each possible split.\n",
        "3. Choose the best split: Choose the split with the highest Information Gain.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have a dataset with two features (A and B) and a target variable. We can calculate the Information Gain for each feature and choose the feature with the highest Information Gain as the best split.\n",
        "\n",
        "| Feature A | Feature B | Target |\n",
        "| --------- | --------- | ------ |\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 0 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 1 |\n",
        "\n",
        "\n",
        "Let's say the entropy of the parent node is 1.0. After splitting on Feature A, the entropy of the child nodes is 0.0 and 0.0, resulting in an Information Gain of 1.0. After splitting on Feature B, the entropy of the child nodes is 0.5 and 0.5, resulting in an Information Gain of 0.5. In this case, Feature A would be chosen as the best split."
      ],
      "metadata": {
        "id": "GEtESgn9bkQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree classifier using entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Print the feature importances\n",
        "for i in range(len(iris.feature_names)):\n",
        "    print(f\"Feature {iris.feature_names[i]}: {feature_importances[i]:.3f}\")\n",
        "\n",
        "\n",
        "# This example demonstrates how to use entropy as the criterion for a Decision Tree classifier in scikit-learn, which is based on Information Gain."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAw-OqX6cSWT",
        "outputId": "5ecb360b-afe8-42b8-dd8f-e246763a9d5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature sepal length (cm): 0.000\n",
            "Feature sepal width (cm): 0.014\n",
            "Feature petal length (cm): 0.895\n",
            "Feature petal width (cm): 0.090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- **Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision Trees have numerous applications in various fields, including:\n",
        "\n",
        "1. Credit Risk Assessment: Decision Trees can be used to evaluate the creditworthiness of loan applicants based on factors such as credit history, income, and employment status.\n",
        "2. Medical Diagnosis: Decision Trees can be used to diagnose diseases based on symptoms, medical history, and test results.\n",
        "3. Customer Segmentation: Decision Trees can be used to segment customers based on demographic and behavioral characteristics, such as age, income, and purchase history.\n",
        "4. Predictive Maintenance: Decision Trees can be used to predict equipment failures and schedule maintenance based on sensor data and historical maintenance records.\n",
        "5. Image Classification: Decision Trees can be used as a component of image classification systems, such as object detection and facial recognition.\n",
        "\n",
        "**Advantages of Decision Trees**\n",
        "\n",
        "1. Interpretability: Decision Trees are easy to understand and interpret, making them a popular choice for applications where transparency is important.\n",
        "2. Handling categorical features: Decision Trees can handle categorical features directly, without requiring additional preprocessing.\n",
        "3. Handling missing values: Decision Trees can handle missing values by treating them as a separate category or using imputation methods.\n",
        "4. Fast training: Decision Trees are relatively fast to train, especially compared to more complex models like neural networks.\n",
        "\n",
        "**Limitations of Decision Trees**\n",
        "\n",
        "1. Overfitting: Decision Trees can overfit the training data, especially when the trees are deep or the dataset is small.\n",
        "2. Instability: Small changes in the data can result in significantly different trees, which can make it challenging to interpret the results.\n",
        "3. Limited handling of complex relationships: Decision Trees can struggle to capture complex relationships between features, especially when the relationships are non-linear.\n",
        "4. Greedy algorithm: Decision Trees use a greedy algorithm to select the best split, which can lead to suboptimal solutions.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we want to build a Decision Tree model to predict whether a customer will churn based on their usage patterns and demographic characteristics. We can use a Decision Tree classifier to identify the most important features and predict the likelihood of churn."
      ],
      "metadata": {
        "id": "7XJ4gEkRccSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset (using Iris as a proxy for a classification problem)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# This example demonstrates how to use a Decision Tree classifier in scikit-learn to solve a classification problem."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDooOYhmdGo2",
        "outputId": "b6fac407-bcb5-4697-e461-7f2ebdbf463b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "hauKSRjsdLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Decision Tree Classifier on the Iris Dataset\n",
        "# Here's a Python program that loads the Iris dataset, trains a Decision Tree Classifier using the Gini criterion, and prints the model's accuracy and feature importances.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "for i in range(len(iris.feature_names)):\n",
        "    print(f\"Feature '{iris.feature_names[i]}' importance: {feature_importances[i]:.3f}\")\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. We load the Iris dataset using load_iris() from sklearn.\n",
        "# 2. We split the data into training and test sets using train_test_split().\n",
        "# 3. We create and train a Decision Tree classifier using the Gini criterion.\n",
        "# 4. We make predictions on the test set and evaluate the model's accuracy.\n",
        "# 5. We print the feature importances, which represent the importance of each feature in the dataset.\n",
        "\n",
        "# Output- The output will include the model's accuracy and feature importances. The feature importances will indicate which features are most important for the Decision Tree classifier to make accurate predictions.\n",
        "\n",
        "# Note that the feature importances are calculated based on the Gini impurity criterion, which measures the reduction in impurity after splitting on a feature. The feature importances are normalized to sum up to 1, making it easier to compare the importance of different features."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd1rd7BQeUYW",
        "outputId": "7090dcf4-9625-481e-aeff-8f1d03b91abd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature 'sepal length (cm)' importance: 0.000\n",
            "Feature 'sepal width (cm)' importance: 0.017\n",
            "Feature 'petal length (cm)' importance: 0.906\n",
            "Feature 'petal width (cm)' importance: 0.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "nMvGh6P7fBrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Decision Tree Classifier with Limited Depth\n",
        "# Here's a Python program that loads the Iris dataset, trains a Decision Tree Classifier with max_depth=3, and compares its accuracy to a fully-grown tree.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a fully-grown Decision Tree classifier\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(\"Fully-grown Tree Accuracy:\", accuracy_full)\n",
        "\n",
        "# Create and train a Decision Tree classifier with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(\"Limited Tree (max_depth=3) Accuracy:\", accuracy_limited)\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"Difference in Accuracy:\", accuracy_full - accuracy_limited)\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. We load the Iris dataset using load_iris() from sklearn.\n",
        "# 2. We split the data into training and test sets using train_test_split().\n",
        "# 3. We create and train a fully-grown Decision Tree classifier.\n",
        "# 4. We create and train a Decision Tree classifier with max_depth=3.\n",
        "# 5. We make predictions on the test set and evaluate the accuracy of both models.\n",
        "# 6. We compare the accuracies of the two models.\n",
        "\n",
        "# Output- The output will include the accuracy of the fully-grown tree and the limited tree, as well as the difference in accuracy between the two models. This will help us understand the impact of limiting the tree's depth on its performance."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5udWK73fORR",
        "outputId": "5bbfdc94-f774-4bd0-f61e-74dff68c4883"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown Tree Accuracy: 1.0\n",
            "Limited Tree (max_depth=3) Accuracy: 1.0\n",
            "Difference in Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the California Housing dataset from sklearn\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "aqwo0C38fft4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Decision Tree Regressor on the California Housing Dataset\n",
        "# Here's a Python program that loads the California Housing dataset, trains a Decision Tree Regressor, and prints the Mean Squared Error (MSE) and feature importances.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = regressor.feature_importances_\n",
        "for i in range(len(cal_housing.feature_names)):\n",
        "    print(f\"Feature '{cal_housing.feature_names[i]}' importance: {feature_importances[i]:.3f}\")\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. We load the California Housing dataset using fetch_california_housing() from sklearn.\n",
        "# 2. We split the data into training and test sets using train_test_split().\n",
        "# 3. We create and train a Decision Tree Regressor.\n",
        "# 4. We make predictions on the test set and evaluate the model's performance using the Mean Squared Error (MSE).\n",
        "# 5. We print the feature importances, which represent the importance of each feature in the dataset.\n",
        "\n",
        "# Output- The output will include the Mean Squared Error (MSE) and feature importances. The feature importances will indicate which features are most important for the Decision Tree Regressor to make accurate predictions.\n",
        "\n",
        "# Note that the feature importances are calculated based on the reduction in impurity after splitting on a feature, and the feature importances are normalized to sum up to 1."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfh9SVYNfvZe",
        "outputId": "ccffdad0-ff14-41c6-e269-e61773d3b0d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature 'MedInc' importance: 0.529\n",
            "Feature 'HouseAge' importance: 0.052\n",
            "Feature 'AveRooms' importance: 0.053\n",
            "Feature 'AveBedrms' importance: 0.029\n",
            "Feature 'Population' importance: 0.031\n",
            "Feature 'AveOccup' importance: 0.131\n",
            "Feature 'Latitude' importance: 0.094\n",
            "Feature 'Longitude' importance: 0.083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "QXnuH1iugGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning Decision Tree Parameters using GridSearchCV\n",
        "# Here's a Python program that loads the Iris dataset, tunes the Decision Tree's max_depth and min_samples_split using GridSearchCV, and prints the best parameters and the resulting model accuracy.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best cross-validation score\n",
        "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. We load the Iris dataset using load_iris() from sklearn.\n",
        "# 2. We split the data into training and test sets using train_test_split().\n",
        "# 3. We define the parameter grid for max_depth and min_samples_split.\n",
        "# 4. We perform grid search using GridSearchCV to find the best combination of parameters.\n",
        "# 5. We print the best parameters and the best cross-validation score.\n",
        "# 6. We evaluate the best model on the test set and print its accuracy.\n",
        "\n",
        "# Output- The output will include the best parameters, the best cross-validation score, and the test set accuracy. This will help us understand the impact of tuning the Decision Tree's parameters on its performance."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYRZagLRgXpH",
        "outputId": "e5a320f0-8a41-43f5-ac53-9d0b03c9cf92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best Cross-Validation Score: 0.9416666666666668\n",
            "Test Set Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "rM_wc5i5grv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step-by-Step Process for Predicting Disease Using Decision Trees\n",
        "# Here's a step-by-step process to handle missing values, encode categorical features, train a Decision Tree model, tune its hyperparameters, and evaluate its performance.\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# 1. Identify missing values: Use isnull() or isna() to detect missing values in the dataset.\n",
        "# 2. Determine the type of missing values: Decide whether the missing values are Missing At Random (MAR), Missing Completely At Random (MCAR), or Missing Not At Random (MNAR).\n",
        "# 3. Choose an imputation strategy: Based on the type of missing values and the dataset, choose an imputation strategy such as mean, median, mode, or imputation using a machine learning model.\n",
        "# 4. Impute missing values: Use the chosen imputation strategy to fill in the missing values.\n",
        "\n",
        "# Example code:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Assume 'df' is the DataFrame with missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['feature1', 'feature2']] = imputer.fit_transform(df[['feature1', 'feature2']])\n",
        "\n",
        "\n",
        "# Step 2: Encode Categorical Features\n",
        "# 1. Identify categorical features: Determine which features are categorical and need encoding.\n",
        "# 2. Choose an encoding strategy: Decide on an encoding strategy such as one-hot encoding, label encoding, or ordinal encoding.\n",
        "# 3. Encode categorical features: Use the chosen encoding strategy to transform categorical features into numerical features.\n",
        "\n",
        "# Example code:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assume 'df' is the DataFrame with categorical features\n",
        "encoder = OneHotEncoder()\n",
        "encoded_features = encoder.fit_transform(df[['categorical_feature']])\n",
        "\n",
        "\n",
        "# Step 3: Train a Decision Tree Model\n",
        "# 1. Split the data: Split the preprocessed data into training and test sets.\n",
        "# 2. Train a Decision Tree model: Use the training data to train a Decision Tree classifier.\n",
        "\n",
        "# Example code:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume 'X' is the feature matrix and 'y' is the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Step 4: Tune Hyperparameters\n",
        "# 1. Define the hyperparameter grid: Determine the hyperparameters to tune and their possible values.\n",
        "# 2. Perform grid search: Use grid search to find the best combination of hyperparameters.\n",
        "\n",
        "# Example code:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Step 5: Evaluate Performance\n",
        "# 1. Evaluate the model: Use the test data to evaluate the performance of the best model.\n",
        "# 2. Calculate metrics: Calculate metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "# Example code:\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Business Value\n",
        "# The Decision Tree model can provide significant business value in the real-world setting by:\n",
        "\n",
        "# 1. Improving diagnosis accuracy: The model can help doctors diagnose diseases more accurately, reducing the risk of misdiagnosis and improving patient outcomes.\n",
        "# 2. Reducing costs: By identifying high-risk patients and providing early intervention, the model can help reduce healthcare costs associated with treating advanced diseases.\n",
        "# 3. Enhancing patient care: The model can help doctors develop personalized treatment plans tailored to individual patients' needs, improving patient care and outcomes.\n",
        "\n",
        "# Overall, the Decision Tree model can be a valuable tool in the healthcare industry, providing insights that can improve diagnosis accuracy, reduce costs, and enhance patient care."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOzo5SQhQh6",
        "outputId": "38ff2c13-0f91-4764-a15a-d5adf9a3265d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}